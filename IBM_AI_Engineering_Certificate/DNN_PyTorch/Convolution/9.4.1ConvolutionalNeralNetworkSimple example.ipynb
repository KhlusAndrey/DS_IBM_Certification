{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neral Network</h5>\n",
    "<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7f8744bc9250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGxCAYAAADLfglZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZiUlEQVR4nO3df2xV9f3H8delyKW49mJhbWlosbgmIAwEygy/mUgTJd2QhDkUxyRZxlJ+1GYbIG44NlrESEwsP1ISmYSAuMwibjNbp67AkFCBSoMbyI/RRtYUHbm3oFws/Xz/2NdLuhao9Fzep+3zkZw/eu7hnrfHcp85517uCTjnnAAAMNDDegAAQPdFhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAnzp9+rRmzZqlvn376mtf+5qmT5+uw4cPW48FeIoIAT50/vx5TZo0SSdOnNDLL7+s1157TZcvX9bUqVN1/Phx6/EAzwT47jjAf37+85/rxRdf1EcffaRBgwZJkiKRiO655x498MAD2rlzp/GEgDc4EwJuwd69exUIBLRjx45Wj23dulWBQEBVVVW3/Pzl5eV64IEHYgGSpOTkZM2aNUtvvvmmmpqabvm5AT8hQsAtmDRpkkaNGqX169e3eqy0tFRjx47V2LFj5ZxTU1NTu5Yvff755zp16pRGjBjR6rlHjBihzz//XKdPn47rfx9wuxAh4BYtXrxYf//731VdXR1bV1VVpaqqKi1cuFCS9Morr+iOO+5o1/KlCxcuyDmnlJSUVvv8ct2nn34a3/844DbpaT0A0FnNmTNHS5cu1fr167V582ZJ0ksvvaSvf/3revTRRyVJ+fn5t3xZLhAI3NJjQGdChIBbFAwG9eMf/1gvvPCCnn/+eX3xxRd67bXXVFRUpGAwKOm/Zy6hUOgrPe9dd92lQCDQ5tnOf/7zn9jzAl0Bl+OADvjJT36iL774Qi+//LI2b96spqYmLViwIPb4rVyOS0xM1De+8Q3V1NS02l9NTY0SExM1ePDg2/LfB8QbZ0JABwwYMECzZ8/Whg0bdOXKFeXn5ysrKyv2+K1ejnvkkUf04osvqq6uTpmZmZKkxsZGvf766/rOd76jnj35q4uugX8nBHTQwYMHdf/990uS/vrXv2ratGkdfs7z589r5MiR6t+/v1atWqVgMKg1a9boyJEjOnjwoIYMGdLhfQB+QIQAD2RnZysxMVEffvihZ8956tQp/fSnP9U777yjpqYmjRs3TmvXrtXo0aM92wdgjXN6oIOOHj2qf/3rX23+m6GOuOeee1ReXu7pcwJ+w5kQcItOnTqls2fP6umnn1Ztba1OnjypPn36WI8FdCp8Og64Rb/+9a81ffp0Xbx4Ub/73e8IEHALOBMCAJjhTAgAYIYIAQDMECEAgBnffUS7ublZ586dU1JSEl/SCACdkHNOjY2NysjIUI8eNz7X8V2Ezp07F/uaEgBA51VXV6eBAwfecBvfXY5LSkqyHgEA4IH2vJ77LkJcggOArqE9r+e+ixAAoPsgQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNxi9CGDRuUnZ2t3r17a8yYMdq7d2+8dgUA6KTiEqGdO3eqsLBQK1as0JEjRzRp0iQ99NBDqq2tjcfuAACdVMA557x+0vvvv1+jR4/Wxo0bY+uGDh2qmTNnqqSkpMW20WhU0Wg09nMkEuFWDgDQBYTDYSUnJ99wG8/PhK5cuaJDhw4pLy+vxfq8vDzt37+/1fYlJSUKhUKxhQABQPfheYQ++eQTXb16VWlpaS3Wp6Wlqb6+vtX2y5cvVzgcji11dXVejwQA8Km43Vn1f+8j4Zxr894SwWBQwWAwXmMAAHzM8zOh/v37KyEhodVZT0NDQ6uzIwBA9+Z5hHr16qUxY8aooqKixfqKigqNHz/e690BADqxuFyOKyoq0hNPPKHc3FyNGzdOZWVlqq2t1YIFC+KxOwBAJxWXCD366KP69NNPtWrVKv373//W8OHD9ac//UmDBg2Kx+4AAJ1UXP6dUEdEIhGFQiHrMQAAHWTy74QAAGgvIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmbjdygHwms++3ANqfcsW4KviTAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAMz2tBwA6m0AgYD2CJMk5Zz0C0GGcCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMCM5xEqKSnR2LFjlZSUpNTUVM2cOVPHjx/3ejcAgC7A8whVVlaqoKBABw4cUEVFhZqampSXl6dLly55vSsAQCcXcHG+Kcn58+eVmpqqyspKTZ48+abbRyIRhUKheI6ETsov98/hfkLX+OVYwJ/C4bCSk5NvuE3cb2oXDoclSSkpKW0+Ho1GFY1GYz9HIpF4jwQA8Im4fjDBOaeioiJNnDhRw4cPb3ObkpIShUKh2JKZmRnPkQAAPhLXy3EFBQX64x//qH379mngwIFtbtPWmRAhQlv8cPlJ8s8lKD8cD78cC/iT6eW4RYsWaffu3dqzZ891AyRJwWBQwWAwXmMAAHzM8wg557Ro0SKVl5frb3/7m7Kzs73eBQCgi/A8QgUFBdq+fbveeOMNJSUlqb6+XpIUCoWUmJjo9e4AAJ2Y5+8JXe8a8ZYtW/TDH/7wpn+ej2jjevzwHojkn/dB/HA8/HIs4E8m7wn54S8GAKBz4LvjAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZuJ+Uzugq+FbQQDvcCYEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgJu4RKikpUSAQUGFhYbx3BQDoZOIaoaqqKpWVlWnEiBHx3A0AoJOKW4QuXryoxx9/XJs3b9Zdd90Vr90AADqxuEWooKBAM2bM0IMPPnjD7aLRqCKRSIsFANA99IzHk7766qs6fPiwqqqqbrptSUmJfvWrX8VjDACAz3l+JlRXV6clS5Zo27Zt6t279023X758ucLhcGypq6vzeiQAgE8FnHPOyyfctWuXHnnkESUkJMTWXb16VYFAQD169FA0Gm3x2P+KRCIKhUJejoQuwuNfVXggEAhYjwAfC4fDSk5OvuE2nl+OmzZtmmpqalqse/LJJzVkyBAtXbr0hgECAHQvnkcoKSlJw4cPb7HuzjvvVL9+/VqtBwB0b3xjAgDAjOfvCXUU7wnhenz2qwrxnhBurD3vCXEmBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMBOX+wkBXZlfviWAb5BAV8CZEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwExP6wGAzsY5Zz2CJCkQCFiPAHQYZ0IAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJm4ROjjjz/W3Llz1a9fP/Xp00f33XefDh06FI9dAQA6Mc+/RfvChQuaMGGCvv3tb+utt95SamqqTp06pb59+3q9KwBAJ+d5hJ577jllZmZqy5YtsXV3332317sBAHQBnl+O2717t3JzczV79mylpqZq1KhR2rx583W3j0ajikQiLRYAQPfgeYROnz6tjRs3KicnR3/+85+1YMECLV68WFu3bm1z+5KSEoVCodiSmZnp9UgAAJ8KOI9vE9mrVy/l5uZq//79sXWLFy9WVVWV3nvvvVbbR6NRRaPR2M+RSIQQoU1+uaOpX3BnVfhdOBxWcnLyDbfx/ExowIABuvfee1usGzp0qGpra9vcPhgMKjk5ucUCAOgePI/QhAkTdPz48RbrTpw4oUGDBnm9KwBAJ+d5hJ566ikdOHBAxcXFOnnypLZv366ysjIVFBR4vSsAQCfn+XtCkvSHP/xBy5cv10cffaTs7GwVFRXpRz/6Ubv+bCQSUSgU8nokdAG8J9QS7wnB79rznlBcItQRRAjX47NfVXNECH5n8sEEAADaiwgBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZntYDAO0VCASsR5AkOeesR5Dkjzn88v8EnRdnQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGc8j1NTUpGeeeUbZ2dlKTEzU4MGDtWrVKjU3N3u9KwBAJ+f5t2g/99xz2rRpk1555RUNGzZM77//vp588kmFQiEtWbLE690BADoxzyP03nvv6bvf/a5mzJghSbr77ru1Y8cOvf/++17vCgDQyXl+OW7ixIl6++23deLECUnSBx98oH379unhhx9uc/toNKpIJNJiAQB0E85jzc3NbtmyZS4QCLiePXu6QCDgiouLr7v9ypUrnSQWlk6z4Brr/xcs/l7C4fDNf4e8/qXcsWOHGzhwoNuxY4c7evSo27p1q0tJSXG//e1v29z+8uXLLhwOx5a6ujrzA8fCcqMF11j/v2Dx99KeCAX+/xfJM5mZmVq2bJkKCgpi637zm99o27Zt+uc//3nTPx+JRBQKhbwcCfCUx39lOjVu740bCYfDSk5OvuE2nr8n9Nlnn6lHj5ZPm5CQwEe0AQCteP7puPz8fK1evVpZWVkaNmyYjhw5onXr1mn+/Ple7woA0Ml5fjmusbFRv/jFL1ReXq6GhgZlZGRozpw5+uUvf6levXrd9M9zOQ5+x+W4a7gchxtpz+U4zyPUUUQIfuezvzKmiBBuxOQ9IQAA2osIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzHj+BaZAvPjl63L88lU1fjkeQEdwJgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAmZ7WAwDtFQgErEfwFY4HugLOhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGDmK0doz549ys/PV0ZGhgKBgHbt2tXiceecnn32WWVkZCgxMVFTp07VsWPHvJoXANCFfOUIXbp0SSNHjlRpaWmbj69du1br1q1TaWmpqqqqlJ6erunTp6uxsbHDwwIAuhjXAZJceXl57Ofm5maXnp7u1qxZE1t3+fJlFwqF3KZNm9p8jsuXL7twOBxb6urqnCQWFhYWlk6+hMPhm3bE0/eEzpw5o/r6euXl5cXWBYNBTZkyRfv372/zz5SUlCgUCsWWzMxML0cCAPiYpxGqr6+XJKWlpbVYn5aWFnvsfy1fvlzhcDi21NXVeTkSAMDH4nJ77/+97bBz7rq3Ig4GgwoGg/EYAwDgc56eCaWnp0tSq7OehoaGVmdHAAB4GqHs7Gylp6eroqIitu7KlSuqrKzU+PHjvdwVAKAL+MqX4y5evKiTJ0/Gfj5z5oyqq6uVkpKirKwsFRYWqri4WDk5OcrJyVFxcbH69Omjxx57zNPBAQBdwFf9WPa7777b5kfx5s2bF/uY9sqVK116eroLBoNu8uTJrqampt3PHw6HzT9WyMLCwsLS8aU9H9EOOOecfCQSiSgUClmPAQDooHA4rOTk5Btuw3fHAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGDGdxHy2b+dBQDcova8nvsuQtwGHAC6hva8nvvua3uam5t17tw5JSUlXfceRDcTiUSUmZmpurq6m35lRFfHsWiJ43ENx+IajsU1XhwL55waGxuVkZGhHj1ufK4Tl5vadUSPHj00cOBAT54rOTm52/9CfYlj0RLH4xqOxTUci2s6eiza+x2gvrscBwDoPogQAMBMl4xQMBjUypUrFQwGrUcxx7FoieNxDcfiGo7FNbf7WPjugwkAgO6jS54JAQA6ByIEADBDhAAAZogQAMAMEQIAmOmSEdqwYYOys7PVu3dvjRkzRnv37rUe6bYrKSnR2LFjlZSUpNTUVM2cOVPHjx+3HssXSkpKFAgEVFhYaD2KiY8//lhz585Vv3791KdPH9133306dOiQ9Vgmmpqa9Mwzzyg7O1uJiYkaPHiwVq1apebmZuvR4m7Pnj3Kz89XRkaGAoGAdu3a1eJx55yeffZZZWRkKDExUVOnTtWxY8c8n6PLRWjnzp0qLCzUihUrdOTIEU2aNEkPPfSQamtrrUe7rSorK1VQUKADBw6ooqJCTU1NysvL06VLl6xHM1VVVaWysjKNGDHCehQTFy5c0IQJE3THHXforbfe0ocffqgXXnhBffv2tR7NxHPPPadNmzaptLRU//jHP7R27Vo9//zzeumll6xHi7tLly5p5MiRKi0tbfPxtWvXat26dSotLVVVVZXS09M1ffp0779k2nUx3/rWt9yCBQtarBsyZIhbtmyZ0UT+0NDQ4CS5yspK61HMNDY2upycHFdRUeGmTJnilixZYj3Sbbd06VI3ceJE6zF8Y8aMGW7+/Pkt1s2aNcvNnTvXaCIbklx5eXns5+bmZpeenu7WrFkTW3f58mUXCoXcpk2bPN13lzoTunLlig4dOqS8vLwW6/Py8rR//36jqfwhHA5LklJSUownsVNQUKAZM2bowQcftB7FzO7du5Wbm6vZs2crNTVVo0aN0ubNm63HMjNx4kS9/fbbOnHihCTpgw8+0L59+/Twww8bT2brzJkzqq+vb/FaGgwGNWXKFM9fS333Ldod8cknn+jq1atKS0trsT4tLU319fVGU9lzzqmoqEgTJ07U8OHDrccx8eqrr+rw4cOqqqqyHsXU6dOntXHjRhUVFenpp5/WwYMHtXjxYgWDQf3gBz+wHu+2W7p0qcLhsIYMGaKEhARdvXpVq1ev1pw5c6xHM/Xl62Vbr6Vnz571dF9dKkJf+t/7EDnnbvneRF3BwoULdfToUe3bt896FBN1dXVasmSJ/vKXv6h3797W45hqbm5Wbm6uiouLJUmjRo3SsWPHtHHjxm4ZoZ07d2rbtm3avn27hg0bpurqahUWFiojI0Pz5s2zHs/c7Xgt7VIR6t+/vxISElqd9TQ0NLQqenexaNEi7d69W3v27PHsPk2dzaFDh9TQ0KAxY8bE1l29elV79uxRaWmpotGoEhISDCe8fQYMGKB77723xbqhQ4fq97//vdFEtn72s59p2bJl+v73vy9J+uY3v6mzZ8+qpKSkW0coPT1d0n/PiAYMGBBbH4/X0i71nlCvXr00ZswYVVRUtFhfUVGh8ePHG01lwzmnhQsX6vXXX9c777yj7Oxs65HMTJs2TTU1Naquro4tubm5evzxx1VdXd1tAiRJEyZMaPVR/RMnTmjQoEFGE9n67LPPWt35MyEhoVt8RPtGsrOzlZ6e3uK19MqVK6qsrPT8tbRLnQlJUlFRkZ544gnl5uZq3LhxKisrU21trRYsWGA92m1VUFCg7du364033lBSUlLs7DAUCikxMdF4utsrKSmp1Xthd955p/r169ft3iN76qmnNH78eBUXF+t73/ueDh48qLKyMpWVlVmPZiI/P1+rV69WVlaWhg0bpiNHjmjdunWaP3++9Whxd/HiRZ08eTL285kzZ1RdXa2UlBRlZWWpsLBQxcXFysnJUU5OjoqLi9WnTx899thj3g7i6WftfGL9+vVu0KBBrlevXm706NHd8mPJktpctmzZYj2aL3TXj2g759ybb77phg8f7oLBoBsyZIgrKyuzHslMJBJxS5YscVlZWa53795u8ODBbsWKFS4ajVqPFnfvvvtum68R8+bNc87992PaK1eudOnp6S4YDLrJkye7mpoaz+fgfkIAADNd6j0hAEDnQoQAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwMz/ATn9cokkJcX3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
